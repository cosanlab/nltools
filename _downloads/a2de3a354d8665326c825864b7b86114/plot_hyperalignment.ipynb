{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Functional Alignment\n\nWhen performing any type of group analysis, we assume that each voxel is\nreflecting the same computations across all participants. This assumption is\nunlikely to be true. Several standard preprocessing steps assist in improving\n'anatomical alignment'. We spatially normalize to a common anatomical template\nand we also apply spatial smoothing to improve signal to noise ratios in a\ntarget voxel by averaging activity in surrounding voxels with a gaussian kernel.\nHowever, these techniques are limited when learning multivariate models, where\nvoxel alignment across participants is critical to making accurate inference.\nThere have been several developments in improving 'functional alignment'.\nJim Haxby's group has pioneered [hyperalignment](https://academic.oup.com/cercor/article/26/6/2919/1754308), which uses an\niterative procrustes transform to scale, rotate, and reflect voxel time series\nso that they are in the same functional space across participants. They have\nfound that this technique can dramatically improve between subject\nclassification accuracy particularly in [ventral temporal cortex](http://haxbylab.dartmouth.edu/publications/HGC+11.pdf). This technique is\nimplemented in the [PyMVPA](http://www.pymvpa.org/) toolbox. Another promising\nfunctional alignment technique known as the [Shared Response Model](http://papers.nips.cc/paper/5855-a-reduced-dimension-fmri-shared-response-model.pdf)\nwas developed at Princeton to improve intersubject-connectivity analyses and is\nimplemented in the [brainiak](https://github.com/brainiak/brainiak) toolbox.\nThey also have found that this technique can improve between subject analyses.\nThis method has several additional interesting properties such as the ability\nto learn a lower dimensional common representational space and also a\nprobabilistic implementation. In this tutorial we demonstrate how to perform\nfunctional alignment using both hyperalignment and the shared response model\nusing nltools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate Data\n\nFirst, let's simulate some data to align. Here we will simulate 3 subjects\nwith 100 data points. Each subject has signal in 30% of the voxels in the\nMPFC with noise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom nltools.mask import create_sphere\nfrom nltools.data import Brain_Data\nimport matplotlib.pyplot as plt\nfrom nilearn.plotting import plot_glass_brain\n\nn_observations = 500\np = .3\nsigma = 1\nn_sub = 3\n\ny = np.zeros(n_observations)\ny[np.arange(75,150)] = 4\ny[np.arange(200,250)] = 10\ny[np.arange(300,475)] = 7\n\ndef simulate_data(n_observations, y, p, sigma, mask):\n    ''' Simulate Brain Data\n\n        Args:\n            n_observations: (int) number of data points\n            y: (array) one dimensional array of signal\n            p: (float) probability of signal in voxels\n            sigma: (float) amount of gaussian noise to add\n\n        Returns:\n            data: (list) of Brain_Data objects\n    '''\n\n    dat = Brain_Data(mask).apply_mask(mask)\n    new_data = np.zeros((dat.shape()[0], n_observations))\n    for i in np.where(dat.data==1)[0]:\n        if np.random.randint(0,high=10) < p:\n            new_data[i,:] = y\n    noise = np.random.randn(new_data.shape[0],n_observations)*sigma\n    dat.data = (new_data+noise).T\n    return dat\n\nmask = create_sphere([0, 45, 0], radius=8)\ndata = [simulate_data(n_observations, y, p, sigma, mask) for x in range(n_sub)]\n\nplt.figure(figsize=(10,3))\nplt.plot(y)\nplt.title('Simulated Signal', fontsize=20)\nplt.xlabel('Time', fontsize=18)\nplt.ylabel('Signal', fontsize=18)\nplot_glass_brain(data[0].mean().to_nifti())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperalign Data\n\nWe will now align voxels with the same signal across participants. We will\nstart using hyperalignment with the procrustes transform. The align function\ntakes a list of Brain_Data objects (or numpy matrices) and aligns voxels based\non similar responses over time. The function outputs a dictionary with keys\nfor a list of the transformed data, corresponding transofmration matrices and\nscaling terms.  In addition it returns the \"common model\" in which all\nsubjects are projected. The disparity values correspond to the multivariate\ndistance of the subject to the common space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nltools.stats import align\n\nout = align(data, method='procrustes')\n\nprint(out.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Transformed Data\n\nTo make it more clear what it is happening we plot the voxel by time matrices\nseparately for each subject. It is clear that there is a consistent signal\nacross voxels, but that the signal is distributed across 'different' voxels.\nThe transformed data shows the voxels for each subject aligned to the common\nspace.  This now permits inferences across the voxels.  As an example, we\nplot the matrices of the original compared to the aligned data across subjects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f,a = plt.subplots(nrows=2, ncols=3, figsize=(15,5), sharex=True, sharey=True)\n[a[0,i].imshow(x.data.T, aspect='auto') for i,x in enumerate(data)]\n[a[1,i].imshow(x.data.T, aspect='auto') for i,x in enumerate(out['transformed'])]\na[0,0].set_ylabel('Original Voxels', fontsize=16)\na[1,0].set_ylabel('Aligned Features', fontsize=16)\n[a[1,x].set_xlabel('Time', fontsize=16) for x in range(3)]\n[a[0,x].set_title('Subject %s' % str(x+1), fontsize=16) for x in range(3)]\nplt.tight_layout()\n\nf,a = plt.subplots(ncols=2, figsize=(15,5), sharex=True, sharey=True)\na[0].imshow(np.mean(np.array([x.data.T for x in data]), axis=0), aspect='auto')\na[1].imshow(np.mean(np.array([x.data.T for x in out['transformed']]), axis=0), aspect='auto')\na[0].set_ylabel('Voxels', fontsize=16)\n[a[x].set_xlabel('Time', fontsize=16) for x in range(2)]\na[0].set_title('Average Voxel x Time Matrix of Original Data', fontsize=16)\na[1].set_title('Average Voxel x Time Matrix of Aligned Data', fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform aligned data back into original subject space\n\nThe transformation matrices can be used to project each subject's aligned\ndata into the original subject specific voxel space. The procrustes method\ndoesn't look identical as there are a few processing steps that occur within\nthe algorithm that would need to be accounted for to fully recover the original\ndata (e.g., centering, and scaling by norm).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backprojected = [np.dot(t.data, tm.data) for t,tm, in zip(out['transformed'], out['transformation_matrix'])]\n\nf,a = plt.subplots(nrows=3, ncols=3, figsize=(15,10), sharex=True, sharey=True)\n[a[0, i].imshow(x.data.T, aspect='auto') for i, x in enumerate(data)]\n[a[1, i].imshow(x.data.T, aspect='auto') for i, x in enumerate(out['transformed'])]\n[a[2, i].imshow(x.T, aspect='auto') for i, x in enumerate(backprojected)]\n[a[i, 0].set_ylabel(x,fontsize=16) for i, x in enumerate(['Original Voxels','Aligned Features', 'Backprojected Voxels'])]\n[a[2, x].set_xlabel('Time', fontsize=16) for x in range(3)]\n[a[0, x].set_title('Subject %s' % str(x+1), fontsize=16) for x in range(3)]\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Align new subject to common model\n\nWe can also align a new subject to the common model without retraining the\nentire model.  Here we individually align subject 3 to the common space\nlearned above.  We also backproject the transformed subject's data into the\noriginal subject voxel space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d3 = data[2]\nd3_out = d3.align(out['common_model'], method='procrustes')\nbp = np.dot(d3_out['transformed'].data, d3_out['transformation_matrix'].data)\n\nf,a = plt.subplots(ncols=3, figsize=(15,5), sharex=True, sharey=True)\na[0].imshow(d3.data.T, aspect='auto')\na[1].imshow(d3_out['transformed'].data.T, aspect='auto')\na[2].imshow(bp.T, aspect='auto')\n[a[i].set_title(x,fontsize=18) for i, x in enumerate(['Original Data',' Transformed_Data', 'Backprojected Data'])]\n[a[x].set_xlabel('Time', fontsize=16) for x in range(2)]\na[0].set_ylabel('Voxels', fontsize=16)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Align subjects in lower dimensional common space\n\nThe shared response model allows for the possibility of aligning in a lower\ndimensional functional space. Here we provide an example of aligning to a 10\ndimensional features space. Previous work has found that this can potentially\nimprove generalizability of multivariate models trained on an ROI compared to\nusing as many features as voxels. This feature is not yet implemented for\nprocrustes transformation as dimensionality reduction would need to happen\neither before or after alignment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_features = 10\nout = align(data, method='probabilistic_srm', n_features=n_features)\n\nbackprojected = [np.dot(t, tm.data) for t,tm in zip(out['transformed'],out['transformation_matrix'])]\n\nf,a = plt.subplots(nrows=3, ncols=3, figsize=(15,10), sharex=True, sharey=False)\n[a[0, i].imshow(x.data.T, aspect='auto') for i, x in enumerate(data)]\n[a[1, i].imshow(x.T, aspect='auto') for i, x in enumerate(out['transformed'])]\n[a[2, i].imshow(x.T, aspect='auto') for i, x in enumerate(backprojected)]\n[a[i, 0].set_ylabel(x, fontsize=16) for i, x in enumerate(['Original Voxels','Aligned Features', 'Backprojected Voxels'])]\n[a[2, x].set_xlabel('Time', fontsize=16) for x in range(3)]\n[a[0, x].set_title('Subject %s' % str(x+1), fontsize=16) for x in range(3)]\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}